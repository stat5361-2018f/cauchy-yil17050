---
title: "Optimization HW3"
author: "Yichu Li.  "
date: "9/19/2018"
output: pdf_document
        
abstract: |
   This project is about using various optimization techniques, such as Newton-Raphson, Fisher’s Scoring, Fixed point method in trying to maximize likelihood of Cauchy distribution functions. Also needs to compare speed and stability of these techniques.
---

```{r, echo = FALSE, warining = FALSE}
need.packages <- function(pkg, ...)
{
  new.pkg <- pkg[! (pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg))
    install.packages(new.pkg, repos = "https://cloud.r-project.org")
  foo <- function(a, ...) suppressMessages(require(a, ...))
  sapply(pkg, foo, character.only = TRUE)
  invisible(NULL)
}

pkgs <- c("elliptic","knitr")
need.packages(pkgs)
```

# Question 1 
##  Log-likelihood function and Fisher information

   $$L(\theta) = p_1p_2{\dots}p_n
   =lnL(\theta) = ln(p_1) + \dots + ln(p_n)$$
   $$l(\theta) = -\sum_{i = 1}^{n}ln\pi - \sum_{i = 1}^{n}ln[1 + (x_i - \theta)^2] =l(\theta) = -nln\pi- \sum_{i = 1}^{n}ln[1 + (x_i - \theta)^2] $$
   $$l'(\theta) = -0 - \sum_{i = 1}^{n}\frac{2(\theta - x_i)}{1 + (\theta - x_i) ^2}=l'(\theta) = -2\sum_{i = 1} ^ {n}\frac{(\theta - x_i)}{1 + (\theta - x_i)^2} $$    
   $$l''(\theta) = -2\sum_{i = 1}^{n}
   \frac{1 + (\theta - x_i)^2 - 2(\theta - x_i)^2}
   {[1 + (\theta - x_i) ^ 2]^2} =l''(\theta) = -2\sum_{i = 1}^{n}\frac{1 - (\theta - x_i)^2}{[1 + (\theta - x_i)^2]^2}$$ 
   $$p'(x) = -\frac{2(x - \theta)}{\pi[1 + (x - \theta)^2]^2}=I(\theta) = \frac{4n}{\pi}\int ^ {\infty}_{-\infty}\frac{(x - \theta)^2}{[1 + (x - \theta)^2]^3}dx$$
   $$I(\theta) = \frac{4n}{\pi}\int^{\infty}_{-\infty}\frac{x^2}{[1 + x^2]^3}dx$$
Let $x = tan\theta$, $1 + x^2 = \frac{1}{cos^2\theta}$, then:
   $$I(\theta) = \frac{4n}{\pi}\int^{\infty}_{-\infty}\frac{sin^2\theta}{cos^2\theta}cos^6 \theta d(tan\theta)=I(\theta) = \frac{4n}{\pi}\int^{\infty}_{-\infty}\frac{1}{cos^2 \theta}sin^2 \theta cos^4 \theta d \theta$$
    $$I(\theta) = \frac{4n}{\pi}\int^{\frac{\pi}{2}}_{-\frac{\pi}{2}}\frac{sin^22 \theta}{4}d \theta=I(\theta) = \frac{4n}{\pi}\frac{\pi}{8} = n/2$$
   

# Question 2
##  Loglikelihood function and plot

Log-likelihood function are given as follows:
$$l(\theta)=-nln\pi-\sum_{i=1}^{n}ln[1+(x_i-\theta)^2]$$



```{r}
set.seed(20180909)
y <- rcauchy(10, 5)

loglike <- function(x, y){
  loglike <- 0
  for (i in 1:length(y)) 
  {
    loglike <- loglike -log(pi) - log(1 + (x - y[i])^2)
  }
  loglike
}


curve(loglike(x,y), from=-30,to=40, n=1000 , xlab="x", ylab="log.like")
```

# Question 3
##  Newton–Raphson method
```{r}



set.seed(20180909)



sample <- rcauchy(10, 5)

x <- sample
f <- function(theta)
{
  -2*sum((theta-x)/(1+(theta-x)^2))
}

fdash <- function(theta)
{
  -2*sum((1-(theta-x)^2)/((1+(theta-x)^2))^2)
}

g=function(theta){theta-f(theta)/fdash(theta)}
s.p <- seq(-10,20,by=0.5)

xold=5.5
  
xnew <- numeric(length(s.p))
for (i in 1:length(s.p)) {
  xtemp <- g(xold)
  xnew[i] <- xtemp
  xold  <- xtemp
}
final_x <- cbind(s.p, xnew)
final_x

plot(final_x[,1], final_x[,2], type = "l")

uniroot(f,c(1,30))

```

The choice for inital point of theta  has great impact of the function for if it is converging or not. After I tries several theta, when theta_1 is smaller than 3.68, the function tend to be extremely small when x become bigger. 

# Question 4
##  Fixed-point iterations method
```{r}
p.fixed <- function(p0,alpha,obs,tol = 1E-6,max.iter = 1000,verbose = F){
  pold <- p0
  pnew <- pold + alpha * (-2)*sum((pold-obs)/(1+(pold-obs)^2))
  iter <- 1
  while ((abs(pnew - pold) > tol) && (iter < max.iter)){
    pold <- pnew
    pnew <-  pold + alpha * (-2)*sum((pold-obs)/(1+(pold-obs)^2))
    iter <- iter + 1
    if(verbose)
      cat("At iteration", iter, "value of p is:", pnew, "\n")
  }
  if (abs(pnew - pold) > tol) {
    cat("Algorithm failed to converge \n")
    return(c("Failed to Converge"))
  }
  else {
    cat("Algorithm converged, in :" ,iter,"iterations \n")
    return(pnew)
  }
}

set.seed(20180909)
sample <- rcauchy(10, 5)
x <- sample

s.p <- seq(-10,20,by=0.5)


p.fixed(p0 = -1,alpha = 1,obs = x)
p.fixed(p0 = -1,alpha = 0.64,obs = x)
p.fixed(p0 = -1,alpha = 0.25,obs = x)
#alpha = 1 will not converge.

s.p <- seq(-10,20,by=0.5)
alpha <- c(1, 0.64, 0.25)
i <- 1
for (i in 1:length("s.p")){
  j <- 1
  for (j in 1:3){
    result <- p.fixed(p0 = s.p[i],alpha = alpha[j],obs = x)
    print(paste0("For starting point ",s.p[i], ", Alpha ", 
                 alpha[j], ". Fix-point result is ", result,".")) 
  }
}

```

# Question 5
##  Fisher scoring method
```{r}
set.seed(20180909)
sample <- rcauchy(10, 5)
x <- sample

s.p <- seq(-10,20,by=0.5)

i <- 1

for (i in 1:length(s.p)) {
  f <- function(theta){length(x)*log(pi)+sum(log(1+(theta-x)^2))}
  grf <- function(theta){2*sum((theta-x)/(1+(theta-x)^2))}
  fs <- function(theta){matrix(length(x)/2,nrow = 1)}
  print(paste0("starting point =",s.p[i]))
  z <- nlminb(s.p[i],f,grf,fs)
  print(paste0("Fisher scoring result is ",z$par))
  f <- function(theta){-2*sum((theta-x)/(1+(theta-x)^2))}
  fdash <- function(theta){-2*sum((1-(theta-x)^2)/((1+(theta-x)^2))^2)}
  result <- newton_raphson(z$par, f, fdash, maxiter = 1000)
  print(paste0("Newton-Rasphon result is ", result$root))
}

```

# Question 6
##  Comment


The project is comparing the the efficiency and stability of Newton-Raphson method, fixed-point method and scoring-Newton-Raphson method when appling those techniques to find the MLE estimator of Cauchy distribution. 

Fixed point iteration is very unstable. It depends on the choice of the alpha vvery much to have the result converage. However, Newton Raphson and Fisher Newton method are both stable. The number of iteration for Fisher Newton method is samller than the one with just Newton method. Thus, Fisher Newton mehtod is the best one with efficiency and stability. 
